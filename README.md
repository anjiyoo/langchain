# langchain
대규모 언어 모델(LLM)을 활용한 애플리케이션 개발을 위한 프레임워크
LLM은 방대한 양의 텍스트와 코드 데이터를 학습하여 인간 수준의 이해와 생성 능력을 갖춘 인공지능 모델
LLM과의 통합: 랭체인은 다양한 LLM 모델과의 통합을 지원
모듈화: 랭체인은 다양한 기능을 제공하는 모듈로 구성
사용 편의성: 랭체인은 사용하기 쉬운 API를 제공
챗복, 문서 생성, 코드 생성, 번역 등에 활용

# faiss
Facebook AI Research(FB AIR)에 의해 개발된 효율적인 유사성 검색 및 클러스터링의 대규모 데이터셋을 위한 라이브러리
벡터 간의 유사성 검색에 초점을 맞추고 있으며, 빠른 검색 속도와 높은 정확도를 제공
추천 시스템, 이미지 검색, 데이터 클러스터링, 머신러닝 모델의 이웃 찾기 등 다양한 애플리케이션에 활용

# 임베딩
사람이 이해하는 자연어나 이미지 등의 복잡한 데이터를 컴퓨터가 처리할 수 있는 숫자 형태의 벡터로 변환
단어 임베딩(Word Embedding): 자연어 처리(NLP)에서 가장 널리 사용되는 형태
이미지 임베딩(Image Embedding): 이미지를 고차원 데이터에서 저차원의 벡터로 변환
문장 임베딩(Sentence Embedding): 문장 전체를 하나의 벡터로 표현
Word2Vec: 단어를 벡터로 변환하는 방법 중 하나로, 단어의 문맥을 기반으로 의미를 파악
GloVe(Global Vectors for Word Representation): Word2Vec과 유사하지만, 전체 코퍼스의 통계 정보를 사용하여 단어 벡터를 학습
BERT (Bidirectional Encoder Representations from Transformers): 문장의 전체적인 맥락을 고려하여 단어나 문장의 임베딩을 생성

# 임베딩 보충 설명
자연어 처리의 역사
# 초기 
"I love you" 문자 -> 숫자 -> 벡터(숫자 덩어리)
"I love you" -> I:[1,0,0] love:[0,1,0] you:[0,0,1]
"I love you" -> vector [1,0,0],[0,1,0],[0,0,1]
"you love me" -> vector [0,0,1],[0,1,0],[0,0,?]

# 현재
-> 숫자 -> 벡터
I : [0.17,0.21,0.61,0.69]
love : [0.8,0.3,0.2,0.1]
you : [0.18,0.43,0.7,0.12]
me : [0.16,0.24,0.65,0.71]